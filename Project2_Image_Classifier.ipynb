{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Project2_Image_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CorrAUDJPY/Udacity-Project-2-Image-Classifer-with-Neural-Net/blob/main/Project2_Image_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y00b5TQZnqs_"
      },
      "source": [
        "# Your First AI application\n",
        "\n",
        "Going forward, AI algorithms will be incorporated into more and more everyday applications. For example, you might want to include an image classifier in a smart phone app. To do this, you'd use a deep learning model trained on hundreds of thousands of images as part of the overall application architecture. A large part of software development in the future will be using these types of models as common parts of applications. \n",
        "\n",
        "In this project, you'll train an image classifier to recognize different species of flowers. You can imagine using something like this in a phone app that tells you the name of the flower your camera is looking at. In practice you'd train this classifier, then export it for use in your application. We'll be using [this dataset](http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html) from Oxford of 102 flower categories, you can see a few examples below. \n",
        "\n",
        "<img src='assets/Flowers.png' width=500px>\n",
        "\n",
        "The project is broken down into multiple steps:\n",
        "\n",
        "* Load the image dataset and create a pipeline.\n",
        "* Build and Train an image classifier on this dataset.\n",
        "* Use your trained model to perform inference on flower images.\n",
        "\n",
        "We'll lead you through each part which you'll implement in Python.\n",
        "\n",
        "When you've completed this project, you'll have an application that can be trained on any set of labeled images. Here your network will be learning about flowers and end up as a command line application. But, what you do with your new skills depends on your imagination and effort in building a dataset. For example, imagine an app where you take a picture of a car, it tells you what the make and model is, then looks up information about it. Go build your own dataset and make something new."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKnPjnLAftRV"
      },
      "source": [
        "## Import Resources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dCk6873paNW",
        "outputId": "346dd39a-bd84-4b8c-83bc-d29da1c93d74"
      },
      "source": [
        "# The new version of dataset is only available in the tfds-nightly package.\n",
        "%pip --no cache-dir install tfds-nightly --user\n",
        "!pip install tensorflow --upgrade --user\n",
        "\n",
        "# DON'T MISS TO RESTART THE KERNEL"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Usage:   \n",
            "  /opt/conda/bin/python -m pip <command> [options]\n",
            "\n",
            "ambiguous option: --no (--no-cache-dir, --no-color, --no-input?)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already up-to-date: tensorflow in /root/.local/lib/python3.7/site-packages (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<3,>=2.3.0 in /root/.local/lib/python3.7/site-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<1.19.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing<1.2,>=1.1.1 in /root/.local/lib/python3.7/site-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /root/.local/lib/python3.7/site-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.11.2)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.33.6)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.16.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.4.0,>=2.3.0 in /root/.local/lib/python3.7/site-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /root/.local/lib/python3.7/site-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /root/.local/lib/python3.7/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /root/.local/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /root/.local/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.23.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /root/.local/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.22.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (41.4.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /root/.local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.5\" in /root/.local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /root/.local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /root/.local/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.2)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /root/.local/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /root/.local/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR6MvwYZaYZv"
      },
      "source": [
        "# Import TensorFlow \n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58Et-4LaaYZv"
      },
      "source": [
        "# TODO: Make all other necessary imports\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import pandas as ps\n",
        "import json\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWKF0YOarpCx"
      },
      "source": [
        "## Load the Dataset\n",
        "\n",
        "Here you'll use `tensorflow_datasets` to load the [Oxford Flowers 102 dataset](https://www.tensorflow.org/datasets/catalog/oxford_flowers102). This dataset has 3 splits: `'train'`, `'test'`, and `'validation'`.  You'll also need to make sure the training data is normalized and resized to 224x224 pixels as required by the pre-trained networks.\n",
        "\n",
        "The validation and testing sets are used to measure the model's performance on data it hasn't seen yet, but you'll still need to normalize and resize the images to the appropriate size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO5hwc90aYZv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXISRjfdrrQ6",
        "outputId": "eaba5e66-2104-49e7-fe5d-ed97a6b8f422"
      },
      "source": [
        "# Download data to default local directory \"~/tensorflow_datasets\"\n",
        "!python -m tensorflow_datasets.scripts.download_and_prepare --register_checksums=True --datasets=oxford_flowers102\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-11-24 13:20:55.936878: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "I1124 13:20:57.711565 140407243487104 download_and_prepare.py:200] Running download_and_prepare for dataset(s):\n",
            "oxford_flowers102\n",
            "2020-11-24 13:20:57.720875: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "2020-11-24 13:20:57.775909: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "2020-11-24 13:20:57.817532: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "I1124 13:20:57.852626 140407243487104 dataset_info.py:434] Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: oxford_flowers102/2.1.1\n",
            "2020-11-24 13:20:57.860780: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "2020-11-24 13:20:57.997786: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "2020-11-24 13:20:58.080958: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "I1124 13:20:58.171769 140407243487104 dataset_info.py:361] Load dataset info from /tmp/tmpbx97bv8qtfds\n",
            "I1124 13:20:58.177998 140407243487104 download_and_prepare.py:138] download_and_prepare for dataset oxford_flowers102/2.1.1...\n",
            "I1124 13:20:58.178389 140407243487104 dataset_builder.py:357] Generating dataset oxford_flowers102 (/root/tensorflow_datasets/oxford_flowers102/2.1.1)\n",
            "\u001b[1mDownloading and preparing dataset oxford_flowers102/2.1.1 (download: 328.90 MiB, generated: 331.34 MiB, total: 660.25 MiB) to /root/tensorflow_datasets/oxford_flowers102/2.1.1...\u001b[0m\n",
            "2020-11-24 13:20:58.322545: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "2020-11-24 13:20:58.351678: I tensorflow/core/platform/cloud/google_auth_provider.cc:180] Attempting an empty bearer token since no token was retrieved from files, and GCE metadata check was skipped.\n",
            "Dl Completed...: 0 url [00:00, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[AI1124 13:20:58.400877 140407243487104 download_manager.py:476] Downloading https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz into /root/tensorflow_datasets/downloads/robots.ox.ac.uk_vgg_flowers_102_102flowersoWedSp98maBn1wypsDib6T-q2NVbO40fwvTflmPmQpY.tgz.tmp.f9a1212c082f476382a7b68327cfc77d...\n",
            "Dl Completed...:   0% 0/1 [00:00<?, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[AI1124 13:20:58.402731 140407243487104 download_manager.py:476] Downloading https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat into /root/tensorflow_datasets/downloads/robots.ox.ac.uk_vgg_flowers_102_imagelabelQc558tX8AD-RkJuVyV4EyAI3B3yv3pQFw82vzoHJBkI.mat.tmp.b01a87855078409b8301ea875693f078...\n",
            "Dl Completed...:   0% 0/2 [00:00<?, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[AI1124 13:20:58.407194 140407243487104 download_manager.py:476] Downloading https://www.robots.ox.ac.uk/~vgg/data/flowers/102/setid.mat into /root/tensorflow_datasets/downloads/robots.ox.ac.uk_vgg_flowers_102_setidSMkjURnWabtzYOtl4t1kAcvHb6vlLbDJOlQsTHUux60.mat.tmp.99a84d9ce3f6451fa2e2ff70b5f78e69...\n",
            "Dl Completed...:   0% 0/3 [00:00<?, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/3 [00:00<?, ? url/s]\n",
            "Dl Size...:   0% 0/328 [00:00<?, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/3 [00:00<?, ? url/s]\n",
            "Dl Size...:   0% 0/328 [00:00<?, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  33% 1/3 [00:00<00:00,  2.21 url/s]\n",
            "Dl Size...:   0% 0/328 [00:00<?, ? MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[AI1124 13:20:58.851523 140405931468544 download_manager.py:495] Skipping extraction for /root/tensorflow_datasets/downloads/robots.ox.ac.uk_vgg_flowers_102_imagelabelSQPpQga6wjv3cqrfBkUZFt9WtY_Eg6YtsyqXuCZWZR0.mat (method=NO_EXTRACT).\n",
            "Dl Completed...:  33% 1/3 [00:00<00:00,  2.21 url/s]\n",
            "Dl Size...:   0% 0/328 [00:00<?, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:00<00:00,  2.21 url/s]\n",
            "Dl Size...:   0% 0/328 [00:00<?, ? MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[AI1124 13:20:58.885411 140405923075840 download_manager.py:495] Skipping extraction for /root/tensorflow_datasets/downloads/robots.ox.ac.uk_vgg_flowers_102_setidRrhnj5H9ldPI9P6rgNJxpsg0od2Jb-Kf0-atnOXI3M0.mat (method=NO_EXTRACT).\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   0% 1/328 [00:01<05:42,  1.05s/ MiB]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   1% 2/328 [00:01<05:41,  1.05s/ MiB]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   1% 3/328 [00:01<04:04,  1.33 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   1% 4/328 [00:01<04:04,  1.33 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   2% 5/328 [00:01<02:55,  1.84 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   2% 6/328 [00:01<02:55,  1.84 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   2% 7/328 [00:01<02:54,  1.84 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   2% 8/328 [00:01<02:05,  2.55 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   3% 9/328 [00:01<02:05,  2.55 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   3% 10/328 [00:01<02:04,  2.55 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   3% 11/328 [00:01<02:04,  2.55 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   4% 12/328 [00:01<01:29,  3.51 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   4% 13/328 [00:01<01:29,  3.51 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   4% 14/328 [00:01<01:29,  3.51 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   5% 15/328 [00:01<01:05,  4.78 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   5% 16/328 [00:01<01:05,  4.78 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   5% 17/328 [00:01<01:05,  4.78 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   5% 18/328 [00:01<01:04,  4.78 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   6% 19/328 [00:01<00:48,  6.36 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   6% 20/328 [00:01<00:48,  6.36 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   6% 21/328 [00:01<00:48,  6.36 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   7% 22/328 [00:01<00:48,  6.36 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   7% 23/328 [00:01<00:37,  8.18 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   7% 24/328 [00:01<00:37,  8.18 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:01<00:00,  2.21 url/s]\n",
            "Dl Size...:   8% 25/328 [00:01<00:37,  8.18 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:   8% 26/328 [00:02<00:36,  8.18 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:   8% 27/328 [00:02<00:28, 10.57 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:   9% 28/328 [00:02<00:28, 10.57 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:   9% 29/328 [00:02<00:28, 10.57 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:   9% 30/328 [00:02<00:22, 13.04 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:   9% 31/328 [00:02<00:22, 13.04 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  10% 32/328 [00:02<00:22, 13.04 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  10% 33/328 [00:02<00:18, 15.64 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  10% 34/328 [00:02<00:18, 15.64 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  11% 35/328 [00:02<00:18, 15.64 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  11% 36/328 [00:02<00:16, 18.06 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  11% 37/328 [00:02<00:16, 18.06 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  12% 38/328 [00:02<00:16, 18.06 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  12% 39/328 [00:02<00:14, 20.19 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  12% 40/328 [00:02<00:14, 20.19 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  12% 41/328 [00:02<00:14, 20.19 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  13% 42/328 [00:02<00:12, 22.26 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  13% 43/328 [00:02<00:12, 22.26 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  13% 44/328 [00:02<00:12, 22.26 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  14% 45/328 [00:02<00:11, 23.98 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  14% 46/328 [00:02<00:11, 23.98 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  14% 47/328 [00:02<00:11, 23.98 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  15% 48/328 [00:02<00:11, 25.22 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  15% 49/328 [00:02<00:11, 25.22 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  15% 50/328 [00:02<00:11, 25.22 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  16% 51/328 [00:02<00:10, 26.15 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  16% 52/328 [00:02<00:10, 26.15 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:02<00:00,  2.21 url/s]\n",
            "Dl Size...:  16% 53/328 [00:02<00:10, 26.15 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:02, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  16% 54/328 [00:03<00:10, 26.86 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  17% 55/328 [00:03<00:10, 26.86 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  17% 56/328 [00:03<00:10, 26.86 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  17% 57/328 [00:03<00:09, 27.50 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  18% 58/328 [00:03<00:09, 27.50 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  18% 59/328 [00:03<00:09, 27.50 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  18% 60/328 [00:03<00:09, 27.84 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  19% 61/328 [00:03<00:09, 27.84 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  19% 62/328 [00:03<00:09, 27.84 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  19% 63/328 [00:03<00:09, 27.99 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  20% 64/328 [00:03<00:09, 27.99 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  20% 65/328 [00:03<00:09, 27.99 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  20% 66/328 [00:03<00:09, 28.19 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  20% 67/328 [00:03<00:09, 28.19 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  21% 68/328 [00:03<00:09, 28.19 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  21% 69/328 [00:03<00:09, 28.35 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  21% 70/328 [00:03<00:09, 28.35 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  22% 71/328 [00:03<00:09, 28.35 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  22% 72/328 [00:03<00:09, 28.35 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  22% 73/328 [00:03<00:08, 29.76 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  23% 74/328 [00:03<00:08, 29.76 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  23% 75/328 [00:03<00:08, 29.76 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  23% 76/328 [00:03<00:08, 29.76 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  23% 77/328 [00:03<00:08, 30.95 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  24% 78/328 [00:03<00:08, 30.95 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  24% 79/328 [00:03<00:08, 30.95 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  24% 80/328 [00:03<00:08, 30.95 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  25% 81/328 [00:03<00:08, 27.54 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  25% 82/328 [00:03<00:08, 27.54 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:03<00:00,  2.21 url/s]\n",
            "Dl Size...:  25% 83/328 [00:03<00:08, 27.54 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  26% 84/328 [00:04<00:08, 27.54 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:04, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  26% 85/328 [00:04<00:08, 29.14 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  26% 86/328 [00:04<00:08, 29.14 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  27% 87/328 [00:04<00:08, 29.14 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  27% 88/328 [00:04<00:08, 29.14 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:04, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  27% 89/328 [00:04<00:08, 29.65 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  27% 90/328 [00:04<00:08, 29.65 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  28% 91/328 [00:04<00:07, 29.65 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  28% 92/328 [00:04<00:07, 29.65 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:04, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  28% 93/328 [00:04<00:08, 28.16 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  29% 94/328 [00:04<00:08, 28.16 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  29% 95/328 [00:04<00:08, 28.16 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  29% 96/328 [00:04<00:08, 28.16 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:04, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  30% 97/328 [00:04<00:08, 28.70 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  30% 98/328 [00:04<00:08, 28.70 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  30% 99/328 [00:04<00:07, 28.70 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:04, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  30% 100/328 [00:04<00:07, 28.81 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  31% 101/328 [00:04<00:07, 28.81 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  31% 102/328 [00:04<00:07, 28.81 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:04, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  31% 103/328 [00:04<00:07, 28.76 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  32% 104/328 [00:04<00:07, 28.76 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  32% 105/328 [00:04<00:07, 28.76 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:04, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  32% 106/328 [00:04<00:07, 28.94 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  33% 107/328 [00:04<00:07, 28.94 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  33% 108/328 [00:04<00:07, 28.94 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:04, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  33% 109/328 [00:04<00:07, 28.98 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  34% 110/328 [00:04<00:07, 28.98 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:04<00:00,  2.21 url/s]\n",
            "Dl Size...:  34% 111/328 [00:04<00:07, 28.98 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:04, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  34% 112/328 [00:05<00:07, 28.59 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  34% 113/328 [00:05<00:07, 28.59 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  35% 114/328 [00:05<00:07, 28.59 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:05, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  35% 115/328 [00:05<00:07, 28.72 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  35% 116/328 [00:05<00:07, 28.72 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  36% 117/328 [00:05<00:07, 28.72 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:05, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  36% 118/328 [00:05<00:07, 27.60 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  36% 119/328 [00:05<00:07, 27.60 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  37% 120/328 [00:05<00:07, 27.60 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:05, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  37% 121/328 [00:05<00:07, 28.04 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  37% 122/328 [00:05<00:07, 28.04 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  38% 123/328 [00:05<00:07, 28.04 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:05, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  38% 124/328 [00:05<00:07, 28.28 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  38% 125/328 [00:05<00:07, 28.28 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  38% 126/328 [00:05<00:07, 28.28 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:05, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  39% 127/328 [00:05<00:07, 27.54 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  39% 128/328 [00:05<00:07, 27.54 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  39% 129/328 [00:05<00:07, 27.54 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:05, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  40% 130/328 [00:05<00:07, 27.92 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  40% 131/328 [00:05<00:07, 27.92 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  40% 132/328 [00:05<00:07, 27.92 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  41% 133/328 [00:05<00:06, 27.92 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:05, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  41% 134/328 [00:05<00:06, 28.52 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  41% 135/328 [00:05<00:06, 28.52 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  41% 136/328 [00:05<00:06, 28.52 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  42% 137/328 [00:05<00:06, 28.52 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:05, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  42% 138/328 [00:05<00:06, 28.35 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  42% 139/328 [00:05<00:06, 28.35 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:05<00:00,  2.21 url/s]\n",
            "Dl Size...:  43% 140/328 [00:05<00:06, 28.35 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  43% 141/328 [00:06<00:06, 28.35 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:06, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  43% 142/328 [00:06<00:06, 28.83 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  44% 143/328 [00:06<00:06, 28.83 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  44% 144/328 [00:06<00:06, 28.83 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:06, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  44% 145/328 [00:06<00:06, 28.24 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  45% 146/328 [00:06<00:06, 28.24 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  45% 147/328 [00:06<00:06, 28.24 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  45% 148/328 [00:06<00:06, 28.24 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:06, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  45% 149/328 [00:06<00:06, 28.06 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  46% 150/328 [00:06<00:06, 28.06 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  46% 151/328 [00:06<00:06, 28.06 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  46% 152/328 [00:06<00:06, 28.06 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:06, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  47% 153/328 [00:06<00:06, 28.42 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  47% 154/328 [00:06<00:06, 28.42 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  47% 155/328 [00:06<00:06, 28.42 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  48% 156/328 [00:06<00:06, 28.42 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:06, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  48% 157/328 [00:06<00:05, 28.89 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  48% 158/328 [00:06<00:05, 28.89 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  48% 159/328 [00:06<00:05, 28.89 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:06, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  49% 160/328 [00:06<00:05, 29.12 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  49% 161/328 [00:06<00:05, 29.12 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  49% 162/328 [00:06<00:05, 29.12 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:06, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  50% 163/328 [00:06<00:05, 28.78 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  50% 164/328 [00:06<00:05, 28.78 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  50% 165/328 [00:06<00:05, 28.78 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:06, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  51% 166/328 [00:06<00:05, 29.04 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  51% 167/328 [00:06<00:05, 29.04 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:06<00:00,  2.21 url/s]\n",
            "Dl Size...:  51% 168/328 [00:06<00:05, 29.04 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:06, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  52% 169/328 [00:07<00:05, 28.65 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  52% 170/328 [00:07<00:05, 28.65 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  52% 171/328 [00:07<00:05, 28.65 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:07, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  52% 172/328 [00:07<00:05, 28.95 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  53% 173/328 [00:07<00:05, 28.95 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  53% 174/328 [00:07<00:05, 28.95 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:07, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  53% 175/328 [00:07<00:05, 28.98 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  54% 176/328 [00:07<00:05, 28.98 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  54% 177/328 [00:07<00:05, 28.98 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:07, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  54% 178/328 [00:07<00:05, 28.75 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  55% 179/328 [00:07<00:05, 28.75 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  55% 180/328 [00:07<00:05, 28.75 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  55% 181/328 [00:07<00:05, 28.75 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:07, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  55% 182/328 [00:07<00:05, 28.35 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  56% 183/328 [00:07<00:05, 28.35 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  56% 184/328 [00:07<00:05, 28.35 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  56% 185/328 [00:07<00:05, 28.35 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:07, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  57% 186/328 [00:07<00:05, 28.31 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  57% 187/328 [00:07<00:04, 28.31 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  57% 188/328 [00:07<00:04, 28.31 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  58% 189/328 [00:07<00:04, 28.31 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:07, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  58% 190/328 [00:07<00:04, 28.96 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  58% 191/328 [00:07<00:04, 28.96 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  59% 192/328 [00:07<00:04, 28.96 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:07, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  59% 193/328 [00:07<00:04, 29.22 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  59% 194/328 [00:07<00:04, 29.22 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  59% 195/328 [00:07<00:04, 29.22 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:07, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  60% 196/328 [00:07<00:04, 28.87 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:07<00:00,  2.21 url/s]\n",
            "Dl Size...:  60% 197/328 [00:07<00:04, 28.87 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  60% 198/328 [00:08<00:04, 28.87 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  61% 199/328 [00:08<00:04, 28.87 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:08, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  61% 200/328 [00:08<00:04, 28.81 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  61% 201/328 [00:08<00:04, 28.81 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  62% 202/328 [00:08<00:04, 28.81 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  62% 203/328 [00:08<00:04, 28.81 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:08, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  62% 204/328 [00:08<00:04, 28.89 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  62% 205/328 [00:08<00:04, 28.89 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  63% 206/328 [00:08<00:04, 28.89 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:08, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  63% 207/328 [00:08<00:04, 29.14 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  63% 208/328 [00:08<00:04, 29.14 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  64% 209/328 [00:08<00:04, 29.14 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:08, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  64% 210/328 [00:08<00:04, 28.32 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  64% 211/328 [00:08<00:04, 28.32 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  65% 212/328 [00:08<00:04, 28.32 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:08, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  65% 213/328 [00:08<00:04, 28.27 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  65% 214/328 [00:08<00:04, 28.27 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  66% 215/328 [00:08<00:03, 28.27 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  66% 216/328 [00:08<00:03, 28.27 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:08, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  66% 217/328 [00:08<00:03, 28.49 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  66% 218/328 [00:08<00:03, 28.49 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  67% 219/328 [00:08<00:03, 28.49 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  67% 220/328 [00:08<00:03, 28.49 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:08, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  67% 221/328 [00:08<00:03, 28.59 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  68% 222/328 [00:08<00:03, 28.59 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  68% 223/328 [00:08<00:03, 28.59 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  68% 224/328 [00:08<00:03, 28.59 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:08, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  69% 225/328 [00:08<00:03, 28.78 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:08<00:00,  2.21 url/s]\n",
            "Dl Size...:  69% 226/328 [00:08<00:03, 28.78 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  69% 227/328 [00:09<00:03, 28.78 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  70% 228/328 [00:09<00:03, 28.78 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:09, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  70% 229/328 [00:09<00:03, 29.24 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  70% 230/328 [00:09<00:03, 29.24 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  70% 231/328 [00:09<00:03, 29.24 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  71% 232/328 [00:09<00:03, 29.24 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:09, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  71% 233/328 [00:09<00:03, 28.75 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  71% 234/328 [00:09<00:03, 28.75 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  72% 235/328 [00:09<00:03, 28.75 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  72% 236/328 [00:09<00:03, 28.75 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:09, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  72% 237/328 [00:09<00:03, 28.89 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  73% 238/328 [00:09<00:03, 28.89 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  73% 239/328 [00:09<00:03, 28.89 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:09, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  73% 240/328 [00:09<00:03, 29.00 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  73% 241/328 [00:09<00:03, 29.00 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  74% 242/328 [00:09<00:02, 29.00 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:09, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  74% 243/328 [00:09<00:03, 27.70 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  74% 244/328 [00:09<00:03, 27.70 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  75% 245/328 [00:09<00:02, 27.70 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:09, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  75% 246/328 [00:09<00:02, 28.33 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  75% 247/328 [00:09<00:02, 28.33 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  76% 248/328 [00:09<00:02, 28.33 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:09, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  76% 249/328 [00:09<00:02, 28.21 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  76% 250/328 [00:09<00:02, 28.21 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  77% 251/328 [00:09<00:02, 28.21 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  77% 252/328 [00:09<00:02, 28.21 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:09, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  77% 253/328 [00:09<00:02, 29.25 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:09<00:00,  2.21 url/s]\n",
            "Dl Size...:  77% 254/328 [00:09<00:02, 29.25 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  78% 255/328 [00:10<00:02, 29.25 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:10, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  78% 256/328 [00:10<00:02, 28.57 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  78% 257/328 [00:10<00:02, 28.57 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  79% 258/328 [00:10<00:02, 28.57 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:10, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  79% 259/328 [00:10<00:02, 28.86 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  79% 260/328 [00:10<00:02, 28.86 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  80% 261/328 [00:10<00:02, 28.86 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:10, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  80% 262/328 [00:10<00:02, 27.75 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  80% 263/328 [00:10<00:02, 27.75 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  80% 264/328 [00:10<00:02, 27.75 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:10, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  81% 265/328 [00:10<00:02, 28.28 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  81% 266/328 [00:10<00:02, 28.28 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  81% 267/328 [00:10<00:02, 28.28 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:10, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  82% 268/328 [00:10<00:02, 28.12 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  82% 269/328 [00:10<00:02, 28.12 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  82% 270/328 [00:10<00:02, 28.12 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:10, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  83% 271/328 [00:10<00:02, 27.95 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  83% 272/328 [00:10<00:02, 27.95 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  83% 273/328 [00:10<00:01, 27.95 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:10, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  84% 274/328 [00:10<00:02, 26.40 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  84% 275/328 [00:10<00:02, 26.40 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  84% 276/328 [00:10<00:01, 26.40 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:10, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  84% 277/328 [00:10<00:01, 27.19 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  85% 278/328 [00:10<00:01, 27.19 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  85% 279/328 [00:10<00:01, 27.19 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:10, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  85% 280/328 [00:10<00:01, 27.42 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  86% 281/328 [00:10<00:01, 27.42 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:10<00:00,  2.21 url/s]\n",
            "Dl Size...:  86% 282/328 [00:10<00:01, 27.42 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:10, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  86% 283/328 [00:11<00:01, 27.96 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  87% 284/328 [00:11<00:01, 27.96 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  87% 285/328 [00:11<00:01, 27.96 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:11, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  87% 286/328 [00:11<00:01, 27.70 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  88% 287/328 [00:11<00:01, 27.70 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  88% 288/328 [00:11<00:01, 27.70 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:11, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  88% 289/328 [00:11<00:01, 27.91 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  88% 290/328 [00:11<00:01, 27.91 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  89% 291/328 [00:11<00:01, 27.91 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:11, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  89% 292/328 [00:11<00:01, 27.29 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  89% 293/328 [00:11<00:01, 27.29 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  90% 294/328 [00:11<00:01, 27.29 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:11, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  90% 295/328 [00:11<00:01, 27.07 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  90% 296/328 [00:11<00:01, 27.07 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  91% 297/328 [00:11<00:01, 27.07 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:11, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  91% 298/328 [00:11<00:01, 26.63 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  91% 299/328 [00:11<00:01, 26.63 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  91% 300/328 [00:11<00:01, 26.63 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  92% 301/328 [00:11<00:01, 26.63 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:11, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  92% 302/328 [00:11<00:00, 27.94 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  92% 303/328 [00:11<00:00, 27.94 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  93% 304/328 [00:11<00:00, 27.94 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:11, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  93% 305/328 [00:11<00:00, 27.44 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  93% 306/328 [00:11<00:00, 27.44 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  94% 307/328 [00:11<00:00, 27.44 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:11, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  94% 308/328 [00:11<00:00, 27.74 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  94% 309/328 [00:11<00:00, 27.74 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:11<00:00,  2.21 url/s]\n",
            "Dl Size...:  95% 310/328 [00:11<00:00, 27.74 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:11, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:12<00:00,  2.21 url/s]\n",
            "Dl Size...:  95% 311/328 [00:12<00:00, 28.26 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:12<00:00,  2.21 url/s]\n",
            "Dl Size...:  95% 312/328 [00:12<00:00, 28.26 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:12<00:00,  2.21 url/s]\n",
            "Dl Size...:  95% 313/328 [00:12<00:00, 28.26 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:12, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:12<00:00,  2.21 url/s]\n",
            "Dl Size...:  96% 314/328 [00:12<00:00, 27.82 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:12<00:00,  2.21 url/s]\n",
            "Dl Size...:  96% 315/328 [00:12<00:00, 27.82 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:12<00:00,  2.21 url/s]\n",
            "Dl Size...:  96% 316/328 [00:12<00:00, 27.82 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:12, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:12<00:00,  2.21 url/s]\n",
            "Dl Size...:  97% 317/328 [00:12<00:00, 27.88 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:12<00:00,  2.21 url/s]\n",
            "Dl Size...:  97% 318/328 [00:12<00:00, 27.88 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:12<00:00,  2.21 url/s]\n",
            "Dl Size...:  97% 319/328 [00:12<00:00, 27.88 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:12, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:12<00:00,  2.21 url/s]\n",
            "Dl Size...:  98% 320/328 [00:12<00:00, 27.90 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:12<00:00,  2.21 url/s]\n",
            "Dl Size...:  98% 321/328 [00:12<00:00, 27.90 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:12<00:00,  2.21 url/s]\n",
            "Dl Size...:  98% 322/328 [00:12<00:00, 27.90 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:12<00:00,  2.21 url/s]\n",
            "Dl Size...:  98% 323/328 [00:12<00:00, 27.90 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:12, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:12<00:00,  2.21 url/s]\n",
            "Dl Size...:  99% 324/328 [00:12<00:00, 27.75 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:12<00:00,  2.21 url/s]\n",
            "Dl Size...:  99% 325/328 [00:12<00:00, 27.75 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:12<00:00,  2.21 url/s]\n",
            "Dl Size...:  99% 326/328 [00:12<00:00, 27.75 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:  67% 2/3 [00:12<00:00,  2.21 url/s]\n",
            "Dl Size...: 100% 327/328 [00:12<00:00, 27.75 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:12, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:  67% 2/3 [00:12<00:00,  2.21 url/s]\n",
            "Dl Size...: 100% 328/328 [00:12<00:00, 28.58 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 3/3 [00:12<00:00,  2.15s/ url]\n",
            "Dl Size...: 100% 328/328 [00:12<00:00, 28.58 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 3/3 [00:12<00:00,  2.15s/ url]\n",
            "Dl Size...: 100% 328/328 [00:12<00:00, 28.58 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...:   0% 0/1 [00:12<?, ? file/s]\u001b[A\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 3/3 [00:17<00:00,  2.15s/ url]\n",
            "Dl Size...: 100% 328/328 [00:17<00:00, 28.58 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 100% 1/1 [00:17<00:00, 17.00s/ file]\u001b[A\u001b[A\n",
            "Extraction completed...: 100% 1/1 [00:17<00:00, 17.00s/ file]\n",
            "\n",
            "Dl Size...: 100% 328/328 [00:17<00:00, 19.29 MiB/s]\n",
            "\n",
            "Dl Completed...: 100% 3/3 [00:17<00:00,  5.67s/ url]\n",
            "I1124 13:21:15.401455 140407243487104 dataset_builder.py:970] Generating split train\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/oxford_flowers102/2.1.1.incomplete61ZZYD/oxford_flowers102-train.tfrecord\n",
            "  0% 0/1020 [00:00<?, ? examples/s]I1124 13:21:15.946902 140407243487104 tfrecords_writer.py:226] Done writing /root/tensorflow_datasets/oxford_flowers102/2.1.1.incomplete61ZZYD/oxford_flowers102-train.tfrecord. Shard lengths: [1020]\n",
            "I1124 13:21:15.951766 140407243487104 dataset_builder.py:970] Generating split test\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/oxford_flowers102/2.1.1.incomplete61ZZYD/oxford_flowers102-test.tfrecord\n",
            " 91% 5582/6149 [00:00<00:00, 17544.28 examples/s]I1124 13:21:18.845862 140407243487104 tfrecords_writer.py:226] Done writing /root/tensorflow_datasets/oxford_flowers102/2.1.1.incomplete61ZZYD/oxford_flowers102-test.tfrecord. Shard lengths: [3074, 3075]\n",
            "I1124 13:21:18.873920 140407243487104 dataset_builder.py:970] Generating split validation\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/oxford_flowers102/2.1.1.incomplete61ZZYD/oxford_flowers102-validation.tfrecord\n",
            "  0% 0/1020 [00:00<?, ? examples/s]I1124 13:21:19.342446 140407243487104 tfrecords_writer.py:226] Done writing /root/tensorflow_datasets/oxford_flowers102/2.1.1.incomplete61ZZYD/oxford_flowers102-validation.tfrecord. Shard lengths: [1020]\n",
            "I1124 13:21:19.347278 140407243487104 dataset_builder.py:412] Skipping computing stats for mode ComputeStatsMode.SKIP.\n",
            "\u001b[1mDataset oxford_flowers102 downloaded and prepared to /root/tensorflow_datasets/oxford_flowers102/2.1.1. Subsequent calls will reuse this data.\u001b[0m\n",
            "\u001b[1mname: \"oxford_flowers102\"\n",
            "description: \"The Oxford Flowers 102 dataset is a consistent of 102 flower categories commonly occurring\\nin the United Kingdom. Each class consists of between 40 and 258 images. The images have\\nlarge scale, pose and light variations. In addition, there are categories that have large\\nvariations within the category and several very similar categories.\\n\\nThe dataset is divided into a training set, a validation set and a test set.\\nThe training set and validation set each consist of 10 images per class (totalling 1020 images each).\\nThe test set consists of the remaining 6149 images (minimum 20 per class).\"\n",
            "citation: \"@InProceedings{Nilsback08,\\n   author = \\\"Nilsback, M-E. and Zisserman, A.\\\",\\n   title = \\\"Automated Flower Classification over a Large Number of Classes\\\",\\n   booktitle = \\\"Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing\\\",\\n   year = \\\"2008\\\",\\n   month = \\\"Dec\\\"\\n}\"\n",
            "location {\n",
            "  urls: \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/\"\n",
            "}\n",
            "schema {\n",
            "  feature {\n",
            "    name: \"file_name\"\n",
            "    type: BYTES\n",
            "    domain: \"file_name\"\n",
            "    presence {\n",
            "      min_fraction: 1.0\n",
            "      min_count: 1\n",
            "    }\n",
            "    shape {\n",
            "      dim {\n",
            "        size: 1\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  feature {\n",
            "    name: \"image\"\n",
            "    type: BYTES\n",
            "    presence {\n",
            "      min_fraction: 1.0\n",
            "      min_count: 1\n",
            "    }\n",
            "    shape {\n",
            "      dim {\n",
            "        size: -1\n",
            "      }\n",
            "      dim {\n",
            "        size: -1\n",
            "      }\n",
            "      dim {\n",
            "        size: 3\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  feature {\n",
            "    name: \"label\"\n",
            "    type: INT\n",
            "    presence {\n",
            "      min_fraction: 1.0\n",
            "      min_count: 1\n",
            "    }\n",
            "    shape {\n",
            "      dim {\n",
            "        size: 1\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  string_domain {\n",
            "    name: \"file_name\"\n",
            "    value: \"image_08133.jpg\"\n",
            "    value: \"image_08138.jpg\"\n",
            "    value: \"image_08157.jpg\"\n",
            "    value: \"image_08173.jpg\"\n",
            "    value: \"image_08174.jpg\"\n",
            "    value: \"image_08176.jpg\"\n",
            "    value: \"image_08179.jpg\"\n",
            "    value: \"image_08182.jpg\"\n",
            "    value: \"image_08185.jpg\"\n",
            "    value: \"image_08187.jpg\"\n",
            "  }\n",
            "}\n",
            "splits {\n",
            "  name: \"test\"\n",
            "  statistics {\n",
            "    num_examples: 6149\n",
            "    features {\n",
            "      type: STRING\n",
            "      string_stats {\n",
            "        common_stats {\n",
            "          num_non_missing: 6149\n",
            "          min_num_values: 1\n",
            "          max_num_values: 1\n",
            "          avg_num_values: 1.0\n",
            "          num_values_histogram {\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            type: QUANTILES\n",
            "          }\n",
            "          tot_num_values: 6149\n",
            "        }\n",
            "        unique: 6149\n",
            "        top_values {\n",
            "          value: \"image_08189.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08188.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08186.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08184.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08183.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08181.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08180.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08178.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08172.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08171.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        avg_length: 15.0\n",
            "        rank_histogram {\n",
            "          buckets {\n",
            "            label: \"image_08189.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 1\n",
            "            high_rank: 1\n",
            "            label: \"image_08188.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 2\n",
            "            high_rank: 2\n",
            "            label: \"image_08186.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 3\n",
            "            high_rank: 3\n",
            "            label: \"image_08184.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 4\n",
            "            high_rank: 4\n",
            "            label: \"image_08183.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 5\n",
            "            high_rank: 5\n",
            "            label: \"image_08181.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 6\n",
            "            high_rank: 6\n",
            "            label: \"image_08180.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 7\n",
            "            high_rank: 7\n",
            "            label: \"image_08178.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 8\n",
            "            high_rank: 8\n",
            "            label: \"image_08172.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 9\n",
            "            high_rank: 9\n",
            "            label: \"image_08171.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      path {\n",
            "        step: \"file_name\"\n",
            "      }\n",
            "    }\n",
            "    features {\n",
            "      type: STRING\n",
            "      string_stats {\n",
            "        common_stats {\n",
            "          num_non_missing: 6149\n",
            "          min_num_values: 1\n",
            "          max_num_values: 1\n",
            "          avg_num_values: 1.0\n",
            "          num_values_histogram {\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            type: QUANTILES\n",
            "          }\n",
            "          tot_num_values: 6149\n",
            "        }\n",
            "        unique: 6147\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 2.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 2.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        avg_length: 42333.9453125\n",
            "        rank_histogram {\n",
            "          buckets {\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 2.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 1\n",
            "            high_rank: 1\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 2.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 2\n",
            "            high_rank: 2\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 3\n",
            "            high_rank: 3\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 4\n",
            "            high_rank: 4\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 5\n",
            "            high_rank: 5\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 6\n",
            "            high_rank: 6\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 7\n",
            "            high_rank: 7\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 8\n",
            "            high_rank: 8\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 9\n",
            "            high_rank: 9\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      path {\n",
            "        step: \"image\"\n",
            "      }\n",
            "    }\n",
            "    features {\n",
            "      num_stats {\n",
            "        common_stats {\n",
            "          num_non_missing: 6149\n",
            "          min_num_values: 1\n",
            "          max_num_values: 1\n",
            "          avg_num_values: 1.0\n",
            "          num_values_histogram {\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 614.9\n",
            "            }\n",
            "            type: QUANTILES\n",
            "          }\n",
            "          tot_num_values: 6149\n",
            "        }\n",
            "        mean: 57.81362823223289\n",
            "        std_dev: 26.889661542349568\n",
            "        num_zeros: 20\n",
            "        median: 60.0\n",
            "        max: 101.0\n",
            "        histograms {\n",
            "          buckets {\n",
            "            high_value: 10.1\n",
            "            sample_count: 388.0019\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 10.1\n",
            "            high_value: 20.2\n",
            "            sample_count: 388.00190000000003\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 20.2\n",
            "            high_value: 30.299999999999997\n",
            "            sample_count: 394.1509\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 30.299999999999997\n",
            "            high_value: 40.4\n",
            "            sample_count: 449.4919\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 40.4\n",
            "            high_value: 50.5\n",
            "            sample_count: 855.3258999999999\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 50.5\n",
            "            high_value: 60.599999999999994\n",
            "            sample_count: 621.6638999999999\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 60.599999999999994\n",
            "            high_value: 70.7\n",
            "            sample_count: 418.74690000000004\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 70.7\n",
            "            high_value: 80.8\n",
            "            sample_count: 1187.3719\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 80.8\n",
            "            high_value: 90.89999999999999\n",
            "            sample_count: 812.2829\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 90.89999999999999\n",
            "            high_value: 101.0\n",
            "            sample_count: 633.9619\n",
            "          }\n",
            "        }\n",
            "        histograms {\n",
            "          buckets {\n",
            "            high_value: 16.0\n",
            "            sample_count: 614.9\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 16.0\n",
            "            high_value: 33.0\n",
            "            sample_count: 614.9\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 33.0\n",
            "            high_value: 44.0\n",
            "            sample_count: 614.9\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 44.0\n",
            "            high_value: 50.0\n",
            "            sample_count: 614.9\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 50.0\n",
            "            high_value: 60.0\n",
            "            sample_count: 614.9\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 60.0\n",
            "            high_value: 72.0\n",
            "            sample_count: 614.9\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 72.0\n",
            "            high_value: 76.0\n",
            "            sample_count: 614.9\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 76.0\n",
            "            high_value: 83.0\n",
            "            sample_count: 614.9\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 83.0\n",
            "            high_value: 91.0\n",
            "            sample_count: 614.9\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 91.0\n",
            "            high_value: 101.0\n",
            "            sample_count: 614.9\n",
            "          }\n",
            "          type: QUANTILES\n",
            "        }\n",
            "      }\n",
            "      path {\n",
            "        step: \"label\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  shard_lengths: 3074\n",
            "  shard_lengths: 3075\n",
            "  num_bytes: 260784877\n",
            "}\n",
            "splits {\n",
            "  name: \"train\"\n",
            "  statistics {\n",
            "    num_examples: 1020\n",
            "    features {\n",
            "      type: STRING\n",
            "      string_stats {\n",
            "        common_stats {\n",
            "          num_non_missing: 1020\n",
            "          min_num_values: 1\n",
            "          max_num_values: 1\n",
            "          avg_num_values: 1.0\n",
            "          num_values_histogram {\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            type: QUANTILES\n",
            "          }\n",
            "          tot_num_values: 1020\n",
            "        }\n",
            "        unique: 1020\n",
            "        top_values {\n",
            "          value: \"image_08177.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08175.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08167.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08166.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08165.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08164.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08161.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08154.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08148.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08135.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        avg_length: 15.0\n",
            "        rank_histogram {\n",
            "          buckets {\n",
            "            label: \"image_08177.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 1\n",
            "            high_rank: 1\n",
            "            label: \"image_08175.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 2\n",
            "            high_rank: 2\n",
            "            label: \"image_08167.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 3\n",
            "            high_rank: 3\n",
            "            label: \"image_08166.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 4\n",
            "            high_rank: 4\n",
            "            label: \"image_08165.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 5\n",
            "            high_rank: 5\n",
            "            label: \"image_08164.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 6\n",
            "            high_rank: 6\n",
            "            label: \"image_08161.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 7\n",
            "            high_rank: 7\n",
            "            label: \"image_08154.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 8\n",
            "            high_rank: 8\n",
            "            label: \"image_08148.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 9\n",
            "            high_rank: 9\n",
            "            label: \"image_08135.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      path {\n",
            "        step: \"file_name\"\n",
            "      }\n",
            "    }\n",
            "    features {\n",
            "      type: STRING\n",
            "      string_stats {\n",
            "        common_stats {\n",
            "          num_non_missing: 1020\n",
            "          min_num_values: 1\n",
            "          max_num_values: 1\n",
            "          avg_num_values: 1.0\n",
            "          num_values_histogram {\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            type: QUANTILES\n",
            "          }\n",
            "          tot_num_values: 1020\n",
            "        }\n",
            "        unique: 1020\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        avg_length: 42545.15625\n",
            "        rank_histogram {\n",
            "          buckets {\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 1\n",
            "            high_rank: 1\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 2\n",
            "            high_rank: 2\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 3\n",
            "            high_rank: 3\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 4\n",
            "            high_rank: 4\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 5\n",
            "            high_rank: 5\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 6\n",
            "            high_rank: 6\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 7\n",
            "            high_rank: 7\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 8\n",
            "            high_rank: 8\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 9\n",
            "            high_rank: 9\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      path {\n",
            "        step: \"image\"\n",
            "      }\n",
            "    }\n",
            "    features {\n",
            "      num_stats {\n",
            "        common_stats {\n",
            "          num_non_missing: 1020\n",
            "          min_num_values: 1\n",
            "          max_num_values: 1\n",
            "          avg_num_values: 1.0\n",
            "          num_values_histogram {\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            type: QUANTILES\n",
            "          }\n",
            "          tot_num_values: 1020\n",
            "        }\n",
            "        mean: 50.5\n",
            "        std_dev: 29.443448620476957\n",
            "        num_zeros: 10\n",
            "        median: 51.0\n",
            "        max: 101.0\n",
            "        histograms {\n",
            "          buckets {\n",
            "            high_value: 10.1\n",
            "            sample_count: 109.242\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 10.1\n",
            "            high_value: 20.2\n",
            "            sample_count: 100.06200000000001\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 20.2\n",
            "            high_value: 30.299999999999997\n",
            "            sample_count: 100.06200000000001\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 30.299999999999997\n",
            "            high_value: 40.4\n",
            "            sample_count: 100.06200000000001\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 40.4\n",
            "            high_value: 50.5\n",
            "            sample_count: 100.06200000000001\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 50.5\n",
            "            high_value: 60.599999999999994\n",
            "            sample_count: 101.08200000000001\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 60.599999999999994\n",
            "            high_value: 70.7\n",
            "            sample_count: 100.06200000000001\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 70.7\n",
            "            high_value: 80.8\n",
            "            sample_count: 100.06200000000001\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 80.8\n",
            "            high_value: 90.89999999999999\n",
            "            sample_count: 100.06200000000001\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 90.89999999999999\n",
            "            high_value: 101.0\n",
            "            sample_count: 109.242\n",
            "          }\n",
            "        }\n",
            "        histograms {\n",
            "          buckets {\n",
            "            high_value: 10.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 10.0\n",
            "            high_value: 20.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 20.0\n",
            "            high_value: 30.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 30.0\n",
            "            high_value: 40.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 40.0\n",
            "            high_value: 51.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 51.0\n",
            "            high_value: 61.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 61.0\n",
            "            high_value: 71.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 71.0\n",
            "            high_value: 81.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 81.0\n",
            "            high_value: 91.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 91.0\n",
            "            high_value: 101.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          type: QUANTILES\n",
            "        }\n",
            "      }\n",
            "      path {\n",
            "        step: \"label\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  shard_lengths: 1020\n",
            "  num_bytes: 43474584\n",
            "}\n",
            "splits {\n",
            "  name: \"validation\"\n",
            "  statistics {\n",
            "    num_examples: 1020\n",
            "    features {\n",
            "      type: STRING\n",
            "      string_stats {\n",
            "        common_stats {\n",
            "          num_non_missing: 1020\n",
            "          min_num_values: 1\n",
            "          max_num_values: 1\n",
            "          avg_num_values: 1.0\n",
            "          num_values_histogram {\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            type: QUANTILES\n",
            "          }\n",
            "          tot_num_values: 1020\n",
            "        }\n",
            "        unique: 1020\n",
            "        top_values {\n",
            "          value: \"image_08187.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08185.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08182.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08179.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08176.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08174.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08173.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08157.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08138.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"image_08133.jpg\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        avg_length: 15.0\n",
            "        rank_histogram {\n",
            "          buckets {\n",
            "            label: \"image_08187.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 1\n",
            "            high_rank: 1\n",
            "            label: \"image_08185.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 2\n",
            "            high_rank: 2\n",
            "            label: \"image_08182.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 3\n",
            "            high_rank: 3\n",
            "            label: \"image_08179.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 4\n",
            "            high_rank: 4\n",
            "            label: \"image_08176.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 5\n",
            "            high_rank: 5\n",
            "            label: \"image_08174.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 6\n",
            "            high_rank: 6\n",
            "            label: \"image_08173.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 7\n",
            "            high_rank: 7\n",
            "            label: \"image_08157.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 8\n",
            "            high_rank: 8\n",
            "            label: \"image_08138.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 9\n",
            "            high_rank: 9\n",
            "            label: \"image_08133.jpg\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      path {\n",
            "        step: \"file_name\"\n",
            "      }\n",
            "    }\n",
            "    features {\n",
            "      type: STRING\n",
            "      string_stats {\n",
            "        common_stats {\n",
            "          num_non_missing: 1020\n",
            "          min_num_values: 1\n",
            "          max_num_values: 1\n",
            "          avg_num_values: 1.0\n",
            "          num_values_histogram {\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            type: QUANTILES\n",
            "          }\n",
            "          tot_num_values: 1020\n",
            "        }\n",
            "        unique: 1020\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        top_values {\n",
            "          value: \"__BYTES_VALUE__\"\n",
            "          frequency: 1.0\n",
            "        }\n",
            "        avg_length: 42256.625\n",
            "        rank_histogram {\n",
            "          buckets {\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 1\n",
            "            high_rank: 1\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 2\n",
            "            high_rank: 2\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 3\n",
            "            high_rank: 3\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 4\n",
            "            high_rank: 4\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 5\n",
            "            high_rank: 5\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 6\n",
            "            high_rank: 6\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 7\n",
            "            high_rank: 7\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 8\n",
            "            high_rank: 8\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_rank: 9\n",
            "            high_rank: 9\n",
            "            label: \"__BYTES_VALUE__\"\n",
            "            sample_count: 1.0\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "      path {\n",
            "        step: \"image\"\n",
            "      }\n",
            "    }\n",
            "    features {\n",
            "      num_stats {\n",
            "        common_stats {\n",
            "          num_non_missing: 1020\n",
            "          min_num_values: 1\n",
            "          max_num_values: 1\n",
            "          avg_num_values: 1.0\n",
            "          num_values_histogram {\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            buckets {\n",
            "              low_value: 1.0\n",
            "              high_value: 1.0\n",
            "              sample_count: 102.0\n",
            "            }\n",
            "            type: QUANTILES\n",
            "          }\n",
            "          tot_num_values: 1020\n",
            "        }\n",
            "        mean: 50.5\n",
            "        std_dev: 29.443448620476957\n",
            "        num_zeros: 10\n",
            "        median: 51.0\n",
            "        max: 101.0\n",
            "        histograms {\n",
            "          buckets {\n",
            "            high_value: 10.1\n",
            "            sample_count: 109.242\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 10.1\n",
            "            high_value: 20.2\n",
            "            sample_count: 100.06200000000001\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 20.2\n",
            "            high_value: 30.299999999999997\n",
            "            sample_count: 100.06200000000001\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 30.299999999999997\n",
            "            high_value: 40.4\n",
            "            sample_count: 100.06200000000001\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 40.4\n",
            "            high_value: 50.5\n",
            "            sample_count: 100.06200000000001\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 50.5\n",
            "            high_value: 60.599999999999994\n",
            "            sample_count: 101.08200000000001\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 60.599999999999994\n",
            "            high_value: 70.7\n",
            "            sample_count: 100.06200000000001\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 70.7\n",
            "            high_value: 80.8\n",
            "            sample_count: 100.06200000000001\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 80.8\n",
            "            high_value: 90.89999999999999\n",
            "            sample_count: 100.06200000000001\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 90.89999999999999\n",
            "            high_value: 101.0\n",
            "            sample_count: 109.242\n",
            "          }\n",
            "        }\n",
            "        histograms {\n",
            "          buckets {\n",
            "            high_value: 10.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 10.0\n",
            "            high_value: 20.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 20.0\n",
            "            high_value: 30.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 30.0\n",
            "            high_value: 40.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 40.0\n",
            "            high_value: 51.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 51.0\n",
            "            high_value: 61.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 61.0\n",
            "            high_value: 71.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 71.0\n",
            "            high_value: 81.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 81.0\n",
            "            high_value: 91.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          buckets {\n",
            "            low_value: 91.0\n",
            "            high_value: 101.0\n",
            "            sample_count: 102.0\n",
            "          }\n",
            "          type: QUANTILES\n",
            "        }\n",
            "      }\n",
            "      path {\n",
            "        step: \"label\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  shard_lengths: 1020\n",
            "  num_bytes: 43180278\n",
            "}\n",
            "supervised_keys {\n",
            "  input: \"image\"\n",
            "  output: \"label\"\n",
            "}\n",
            "version: \"2.1.1\"\n",
            "download_size: 344878000\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou1ECdcraYZv"
      },
      "source": [
        "# TODO: Load the dataset with TensorFlow Datasets. Hint: use tfds.load()\n",
        "#dataset,dataset_info=tfds.load('oxford_flowers102', as_supervised=True, with_info=True)\n",
        "\n",
        "(dataset, dataset_info) = tfds.load('oxford_flowers102', as_supervised=True, with_info=True,download=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYmNhrupaYZw",
        "outputId": "e30dbd4f-6f13-40be-8163-c8f68095b25e"
      },
      "source": [
        "dataset_info"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='oxford_flowers102',\n",
              "    version=2.1.1,\n",
              "    description='The Oxford Flowers 102 dataset is a consistent of 102 flower categories commonly occurring\n",
              "in the United Kingdom. Each class consists of between 40 and 258 images. The images have\n",
              "large scale, pose and light variations. In addition, there are categories that have large\n",
              "variations within the category and several very similar categories.\n",
              "\n",
              "The dataset is divided into a training set, a validation set and a test set.\n",
              "The training set and validation set each consist of 10 images per class (totalling 1020 images each).\n",
              "The test set consists of the remaining 6149 images (minimum 20 per class).',\n",
              "    homepage='https://www.robots.ox.ac.uk/~vgg/data/flowers/102/',\n",
              "    features=FeaturesDict({\n",
              "        'file_name': Text(shape=(), dtype=tf.string),\n",
              "        'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
              "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=102),\n",
              "    }),\n",
              "    total_num_examples=8189,\n",
              "    splits={\n",
              "        'test': 6149,\n",
              "        'train': 1020,\n",
              "        'validation': 1020,\n",
              "    },\n",
              "    supervised_keys=('image', 'label'),\n",
              "    citation=\"\"\"@InProceedings{Nilsback08,\n",
              "       author = \"Nilsback, M-E. and Zisserman, A.\",\n",
              "       title = \"Automated Flower Classification over a Large Number of Classes\",\n",
              "       booktitle = \"Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing\",\n",
              "       year = \"2008\",\n",
              "       month = \"Dec\"\n",
              "    }\"\"\",\n",
              "    redistribution_info=,\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wENFvAORaYZw"
      },
      "source": [
        "# TODO: Create a training set, a validation set and a test set.\n",
        "train=dataset['train']\n",
        "validation=dataset['validation']\n",
        "test=dataset['test']\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5pdQnDbf0-j"
      },
      "source": [
        "## Explore the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XikJ4X7FUv8v",
        "outputId": "e85ad264-ff7b-4d38-e3a5-658e055c4462"
      },
      "source": [
        "# TODO: Get the number of examples in each set from the dataset info.\n",
        "train_examples=dataset_info.splits['train'].num_examples\n",
        "validation_examples=dataset_info.splits['validation'].num_examples\n",
        "test_examples=dataset_info.splits['test'].num_examples\n",
        "\n",
        "# TODO: Get the number of classes in the dataset from the dataset info.\n",
        "data_classes=dataset_info.features['label'].num_classes\n",
        "\n",
        "print('Train Set: Num of examples ',train_examples)\n",
        "print('Validation Set: Num of examples ',validation_examples)\n",
        "print('Test Set: Num of examples ',test_examples)\n",
        "print('Total Num of Features ',data_classes)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Set: Num of examples  1020\n",
            "Validation Set: Num of examples  1020\n",
            "Test Set: Num of examples  6149\n",
            "Total Num of Features  102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWR9ScCbPI_D"
      },
      "source": [
        "# TODO: Print the shape and corresponding label of 3 images in the training set.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQbnq8htRTnl"
      },
      "source": [
        "# TODO: Plot 1 image from the training set. \n",
        "\n",
        "# Set the title of the plot to the corresponding image label. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuh1841cs-j1"
      },
      "source": [
        "### Label Mapping\n",
        "\n",
        "You'll also need to load in a mapping from label to category name. You can find this in the file `label_map.json`. It's a JSON object which you can read in with the [`json` module](https://docs.python.org/3.7/library/json.html). This will give you a dictionary mapping the integer coded labels to the actual names of the flowers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoVzdO3KsdSk"
      },
      "source": [
        "with open('label_map.json', 'r') as f:\n",
        "    class_names = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc6pMUZgEvUo"
      },
      "source": [
        "# TODO: Plot 1 image from the training set. Set the title \n",
        "# of the plot to the corresponding class name. \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gL7AaqNf-NC"
      },
      "source": [
        "## Create Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hNznLbPNZxS"
      },
      "source": [
        "# TODO: Create a pipeline for each set.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR9gtRbeXPYx"
      },
      "source": [
        "# Build and Train the Classifier\n",
        "\n",
        "Now that the data is ready, it's time to build and train the classifier. You should use the MobileNet pre-trained model from TensorFlow Hub to get the image features. Build and train a new feed-forward classifier using those features.\n",
        "\n",
        "We're going to leave this part up to you. If you want to talk through it with someone, chat with your fellow students! \n",
        "\n",
        "Refer to the rubric for guidance on successfully completing this section. Things you'll need to do:\n",
        "\n",
        "* Load the MobileNet pre-trained network from TensorFlow Hub.\n",
        "* Define a new, untrained feed-forward network as a classifier.\n",
        "* Train the classifier.\n",
        "* Plot the loss and accuracy values achieved during training for the training and validation set.\n",
        "* Save your trained model as a Keras model. \n",
        "\n",
        "We've left a cell open for you below, but use as many as you need. Our advice is to break the problem up into smaller parts you can run separately. Check that each part is doing what you expect, then move on to the next. You'll likely find that as you work through each part, you'll need to go back and modify your previous code. This is totally normal!\n",
        "\n",
        "When training make sure you're updating only the weights of the feed-forward network. You should be able to get the validation accuracy above 70% if you build everything right.\n",
        "\n",
        "**Note for Workspace users:** One important tip if you're using the workspace to run your code: To avoid having your workspace disconnect during the long-running tasks in this notebook, please read in the earlier page in this lesson called Intro to GPU Workspaces about Keeping Your Session Active. You'll want to include code from the workspace_utils.py module. Also, If your model is over 1 GB when saved as a checkpoint, there might be issues with saving backups in your workspace. If your saved checkpoint is larger than 1 GB (you can open a terminal and check with `ls -lh`), you should reduce the size of your hidden layers and train again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zElEHViXLni"
      },
      "source": [
        "# TODO: Build and train your network.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU6sWzx4e7Yb"
      },
      "source": [
        "# TODO: Plot the loss and accuracy values achieved during training for the training and validation set.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcTDnyvop3ky"
      },
      "source": [
        "## Testing your Network\n",
        "\n",
        "It's good practice to test your trained network on test data, images the network has never seen either in training or validation. This will give you a good estimate for the model's performance on completely new images. You should be able to reach around 70% accuracy on the test set if the model has been trained well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79l7-HM1cafO"
      },
      "source": [
        "# TODO: Print the loss and accuracy values achieved on the entire test set.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLsIDWnuqfkl"
      },
      "source": [
        "## Save the Model\n",
        "\n",
        "Now that your network is trained, save the model so you can load it later for making inference. In the cell below save your model as a Keras model (*i.e.* save it as an HDF5 file)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XOwdOjSptp-"
      },
      "source": [
        "# TODO: Save your trained model as a Keras model.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbeLSRC1rxuj"
      },
      "source": [
        "## Load the Keras Model\n",
        "\n",
        "Load the Keras model you saved above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T6Dgc7Nrzds"
      },
      "source": [
        "# TODO: Load the Keras model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjucwuFrsyhJ"
      },
      "source": [
        "# Inference for Classification\n",
        "\n",
        "Now you'll write a function that uses your trained network for inference. Write a function called `predict` that takes an image, a model, and then returns the top $K$ most likely class labels along with the probabilities. The function call should look like: \n",
        "\n",
        "```python\n",
        "probs, classes = predict(image_path, model, top_k)\n",
        "```\n",
        "\n",
        "If `top_k=5` the output of the `predict` function should be something like this:\n",
        "\n",
        "```python\n",
        "probs, classes = predict(image_path, model, 5)\n",
        "print(probs)\n",
        "print(classes)\n",
        "> [ 0.01558163  0.01541934  0.01452626  0.01443549  0.01407339]\n",
        "> ['70', '3', '45', '62', '55']\n",
        "```\n",
        "\n",
        "Your `predict` function should use `PIL` to load the image from the given `image_path`. You can use the [Image.open](https://pillow.readthedocs.io/en/latest/reference/Image.html#PIL.Image.open) function to load the images. The `Image.open()` function returns an `Image` object. You can convert this `Image` object to a NumPy array by using the `np.asarray()` function.\n",
        "\n",
        "The `predict` function will also need to handle pre-processing the input image such that it can be used by your model. We recommend you write a separate function called `process_image` that performs the pre-processing. You can then call the `process_image` function from the `predict` function. \n",
        "\n",
        "### Image Pre-processing\n",
        "\n",
        "The `process_image` function should take in an image (in the form of a NumPy array) and return an image in the form of a NumPy array with shape `(224, 224, 3)`.\n",
        "\n",
        "First, you should convert your image into a TensorFlow Tensor and then resize it to the appropriate size using `tf.image.resize`.\n",
        "\n",
        "Second, the pixel values of the input images are typically encoded as integers in the range 0-255, but the model expects the pixel values to be floats in the range 0-1. Therefore, you'll also need to normalize the pixel values. \n",
        "\n",
        "Finally, convert your image back to a NumPy array using the `.numpy()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG7mJ1-5s1qe"
      },
      "source": [
        "# TODO: Create the process_image function\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFW3kKf5aYZx"
      },
      "source": [
        "To check your `process_image` function we have provided 4 images in the `./test_images/` folder:\n",
        "\n",
        "* cautleya_spicata.jpg\n",
        "* hard-leaved_pocket_orchid.jpg\n",
        "* orange_dahlia.jpg\n",
        "* wild_pansy.jpg\n",
        "\n",
        "The code below loads one of the above images using `PIL` and plots the original image alongside the image produced by your `process_image` function. If your `process_image` function works, the plotted image should be the correct size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS6UevGfaYZx"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "image_path = './test_images/hard-leaved_pocket_orchid.jpg'\n",
        "im = Image.open(image_path)\n",
        "test_image = np.asarray(im)\n",
        "\n",
        "processed_test_image = process_image(test_image)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(figsize=(10,10), ncols=2)\n",
        "ax1.imshow(test_image)\n",
        "ax1.set_title('Original Image')\n",
        "ax2.imshow(processed_test_image)\n",
        "ax2.set_title('Processed Image')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqjZ8TTJaYZx"
      },
      "source": [
        "Once you can get images in the correct format, it's time to write the `predict` function for making inference with your model.\n",
        "\n",
        "### Inference\n",
        "\n",
        "Remember, the `predict` function should take an image, a model, and then returns the top $K$ most likely class labels along with the probabilities. The function call should look like: \n",
        "\n",
        "```python\n",
        "probs, classes = predict(image_path, model, top_k)\n",
        "```\n",
        "\n",
        "If `top_k=5` the output of the `predict` function should be something like this:\n",
        "\n",
        "```python\n",
        "probs, classes = predict(image_path, model, 5)\n",
        "print(probs)\n",
        "print(classes)\n",
        "> [ 0.01558163  0.01541934  0.01452626  0.01443549  0.01407339]\n",
        "> ['70', '3', '45', '62', '55']\n",
        "```\n",
        "\n",
        "Your `predict` function should use `PIL` to load the image from the given `image_path`. You can use the [Image.open](https://pillow.readthedocs.io/en/latest/reference/Image.html#PIL.Image.open) function to load the images. The `Image.open()` function returns an `Image` object. You can convert this `Image` object to a NumPy array by using the `np.asarray()` function.\n",
        "\n",
        "**Note:** The image returned by the `process_image` function is a NumPy array with shape `(224, 224, 3)` but the model expects the input images to be of shape `(1, 224, 224, 3)`. This extra dimension represents the batch size. We suggest you use the `np.expand_dims()` function to add the extra dimension. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBnPKFJuGB32"
      },
      "source": [
        "# TODO: Create the predict function\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aft8f_n5C7Co"
      },
      "source": [
        "# Sanity Check\n",
        "\n",
        "It's always good to check the predictions made by your model to make sure they are correct. To check your predictions we have provided 4 images in the `./test_images/` folder:\n",
        "\n",
        "* cautleya_spicata.jpg\n",
        "* hard-leaved_pocket_orchid.jpg\n",
        "* orange_dahlia.jpg\n",
        "* wild_pansy.jpg\n",
        "\n",
        "In the cell below use `matplotlib` to plot the input image alongside the probabilities for the top 5 classes predicted by your model. Plot the probabilities as a bar graph. The plot should look like this:\n",
        "\n",
        "<img src='assets/inference_example.png' width=600px>\n",
        "\n",
        "You can convert from the class integer labels to actual flower names using `class_names`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_tBH8xGGVxQ"
      },
      "source": [
        "# TODO: Plot the input image along with the top 5 classes\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}